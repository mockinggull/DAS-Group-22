---
title: "The Influence of Text Characteristics for Email Classification"
author: "Group 22"
number-sections: true
format: 
  html:
    embed-resources: true
    code-tools: true
  pdf: default
editor_options: 
  chunk_output_type: console
execute:
  echo: true
  eval: true
  warning: false
  message: false
---

```{r}
#| label: libraries
library(tibble)
library(ggplot2)
library(dplyr)
library(gt)
library(patchwork)
library(gridExtra)
library(broom)
library(knitr)
library(GGally)
library(sjPlot)
library(MASS)
```

```{r}
#| label: read data
email<-read.csv("dataset22.csv")
```

```{r}
#| label: convert cht to factor
email$yesno<-as.factor(email$yesno)
```

# Introduction

This analysis focuses on identifying the characteristics of email messages that are predictive of spam. With the rise of unwanted and potentially harmful emails, effective spam detection has become a crucial challenge. By analysing patterns in email content, structure, and metadata, we aim to identify the distinguishing features that separate spam from legitimate messages. Gaining a deeper understanding of these characteristics can lead to the development of smarter, more accurate filtering systems that reduce inbox clutter and protect users from scams and other malicious activities.

# Exploratory Data Analysis {#sec-EDA}

## Data Visualization

```{r}
#| label: fig-pairs
#| fig-cap: Correlations between each variables.
ggpairs(email[,1:6]) +
  theme(plot.background = element_rect(
    fill = "transparent",
    colour = NA,
    size = 1))
```

By @fig-pairs, we see that the correlations between each explanatory is not larger than 0.4, which means the relationship between each variable is weak. It suggests that there is not much concern about multicollinearity. From the scatter plot, there are some points are far away from cluster. Those points can be removed as outliers to improve model's stability.

## Removing Outliers

```{r}
#| label: Remove Outliers by z score
remove_outliers_rowwise_zscore <- function(df) {
  df_clean <- df
  for (col_name in names(df)) {
    if (is.numeric(df[[col_name]])) {
      z_scores <- scale(df[[col_name]])
      df_clean <- df_clean[abs(z_scores) <= 3, ]
    }
  }
  return(na.omit(df_clean))
}
email_clean <- remove_outliers_rowwise_zscore(email)
```

To minimize outliers while preserving as much information as possible, z score are used to remove outliers.

## Scaling Data

```{r}
#| label: tbl-range
#| tbl-cap: Range of each variables.
range_table <- apply(email_clean[-7], 2, range)
range_df <- as.data.frame(t(range_table))
colnames(range_df) <- c("Min", "Max")
range_df$Variables<-colnames(email_clean[-7])
gt(range_df[,c(3,1,2)])
```

After removing outliers, the @tbl-range shows the range of each variable. It is clear that the range of the number of total length of uninterrupted sequences of capitals is extremely large than others. Hence, normalization are processed to reduce bias when large scale may dominates the cost function.

```{r}
#| label: tbl-scaled_range
#| tbl-cap: Range of each scaled variables.
email_clean$crl.tot<-scale(email_clean$crl.tot)
range_table <- apply(email_clean[-7], 2, range)
range_df <- as.data.frame(t(range_table))
colnames(range_df) <- c("Min", "Max")
range_df$Variables<-colnames(email_clean[-7])
gt(range_df[, c(3,1,2)]) %>%
  fmt_number(columns = everything(),decimals = 2)
```

After scaling，by @tbl-scaled_range, the range of the number of total length of uninterrupted sequences of capitals is shrank to a range which is close to others.

## Data Visualization

In this section, we analyze the boxplots of key textual features to compare their distributions between spam and non-spam emails. Boxplots provide an insightful visualization of how different variables behave in each category, highlighting key statistics such as medians, interquartile ranges (IQRs), and potential outliers.

```{r}
#| label: fig-boxplot.crl.tot
#| fig-cap: Boxplot of total length of uninterrupted sequences of capitals.
ggplot(email_clean, aes(x = yesno, y = crl.tot)) +
  geom_boxplot() +
  labs(x = "Spam indictor", y = "Uninterrupted sequences of capitals", 
       title = "Spam indictor with total length 
       of uninterrupted sequences of capitals")
```

The first boxplot examines the length of the longest uninterrupted sequence of capital letters. Capitalization is often used in spam emails to emphasize urgency or importance. The @fig-boxplot.crl.tot reveals that spam emails tend to contain significantly longer sequences of capitalized text compared to non-spam emails. The extreme upper range and high number of outliers suggest that spam messages frequently employ long uppercase segments, potentially to highlight promotions or urgent calls to action. This feature is a strong indicator of spam-like behavior.

```{r}
#| label: fig-boxplot.dollar
#| fig-cap: Boxplot of occurrences of the dollar sign.
ggplot(email_clean, aes(x = yesno, y = dollar)) +
  geom_boxplot() +
  labs(x = "Spam indictor", y = "Occurrences of the dollar sign", 
       title = "Spam indictor with occurrences of the dollar sign")
```

Next, we consider the occurrences of the dollar sign (“\$”). The results in @fig-boxplot.dollar indicate that spam emails have a significantly higher median frequency of "\$" than non-spam emails. This suggests that spam messages frequently reference financial incentives, monetary amounts, or sales offers, reinforcing their promotional or fraudulent nature. The boxplot also displays a wide distribution with several extreme outliers, indicating that some spam emails heavily utilize the dollar sign to lure recipients with promises of wealth or investment opportunities.

```{r}
#| label: fig-boxplot.bang
#| fig-cap: Boxplot of occurrences of '!'.
ggplot(email_clean, aes(x = yesno, y = bang)) +
  geom_boxplot() +
  labs(x = "Spam indictor", y = 'Occurrences of "!"', 
       title = 'Spam indictor with occurrences of "!"')
```

The @fig-boxplot.bang investigates the occurrences of the exclamation mark (“!”). The results demonstrate that spam emails contain a much higher median number of exclamation marks than non-spam emails. This aligns with the general tendency of spam messages to use excessive punctuation to create a sense of urgency or excitement. The presence of multiple outliers further emphasizes that some spam emails employ an abnormally high number of exclamation marks, making this a useful feature for distinguishing spam content.

```{r}
#| label: fig-boxplot.money
#| fig-cap: Boxplot of occurrences of "money".
ggplot(email_clean, aes(x = yesno, y = money)) +
  geom_boxplot() +
  labs(x = "Spam indictor", y = 'Occurrences of "money"', 
       title = 'Spam indictor with occurrences of "money"')
```

Following @fig-boxplot.money, we analyze the occurrences of the word “money” in emails. The visualization shows that spam emails have a noticeably higher median count of this word compared to non-spam emails. The interquartile range (IQR) of the spam category is also wider, indicating a greater variability in how frequently "money" appears. Additionally, a number of extreme outliers suggest that certain spam emails heavily emphasize monetary-related language, further distinguishing them from legitimate emails.

```{r}
#| label: fig-boxplot.n000
#| fig-cap: Boxplot of occurrences of '000'.
ggplot(email_clean, aes(x = yesno, y = n000)) +
  geom_boxplot() +
  labs(x = "Spam indictor", y = 'Occurrences of "000"', 
       title = 'Spam indictor with occurrences of "000"')
```

Next, we examine the occurrences of numerical sequences like “000”. The @fig-boxplot.n000 reveals that this feature appears more frequently in spam emails, with a higher median and broader spread. The presence of multiple outliers suggests that certain spam emails are particularly saturated with numerical expressions, possibly to emphasize large sums of money or create a sense of urgency in recipients. This supports the notion that spam emails often exploit numerical values to attract attention.

```{r}
#| label: fig-boxplot.make
#| fig-cap: Boxplot of occurrences of 'make'.
ggplot(email_clean, aes(x = yesno, y = make)) +
  geom_boxplot() +
  labs(x = "Spam indictor", y = 'Occurrences of "make"', 
       title = 'Spam indictor with occurrences of "make"')
```

Lastly, we analyze the occurrences of the word “make”. This word appears more frequently in spam emails, with a noticeably higher median compared to non-spam messages. Given its common use in phrases like "make money fast" or "make quick cash," this result supports the hypothesis that spam emails often contain persuasive or action-driven language. The distribution in @fig-boxplot.make also suggests that some spam emails repeatedly use this word as part of their marketing or fraudulent strategies.

Overall, these boxplots reveal distinct textual patterns in spam emails, reinforcing the idea that spam messages tend to use specific words, symbols, and formatting styles at significantly higher frequencies than legitimate emails. The presence of extreme outliers in certain variables further suggests that some spam messages adopt aggressive tactics to stand out. These findings provide valuable insights for developing spam detection models by incorporating these textual features as predictive indicators.

# Formal Data Analysis

## Fitting Model in Log-odds

```{r}
#| label: fiting model1
model1 <- glm(yesno ~ crl.tot+dollar+bang+money+n000+make, data = email_clean, 
             family = binomial(link = "logit"))
```

```{r}
#| label: summary model1
summary(model1)
mod1coefs <- round(coef(model1), 2)
```

The fitted linear model 1 is \begin{align}
\ln\left(\frac{p}{1-p}\right) &= \alpha + \beta_{crl.tot} \cdot \textrm{crl.tot} +\beta_{dollar} \cdot \textrm{dollar} +\beta_{bang} \cdot \textrm{bang} +\nonumber\\&\hspace{0.5cm}\beta_{money} \cdot \textrm{money} +\beta_{n000} \cdot \textrm{n000}+\beta_{make} \cdot \textrm{make} \nonumber\\&= `r mod1coefs[1]` + `r mod1coefs[2]` \cdot \textrm{crl.tot} +`r mod1coefs[3]` \cdot \textrm{dollar} +`r mod1coefs[4]` \cdot \textrm{bang} +\nonumber\\&`r mod1coefs[5]` \cdot \textrm{money} +`r mod1coefs[6]`\cdot \textrm{n000}+`r mod1coefs[7]`\cdot \textrm{make} \nonumber
\end{align}

```{r}
#| label: fiting model2
model2 <- glm(yesno ~ crl.tot+dollar+bang+money+n000, data = email_clean, 
             family = binomial(link = "logit"))
```

```{r}
#| label: summary model2
summary(model2)
mod2coefs <- round(coef(model2), 3)
```

To interpret this fitted model, firstly we note that the baseline category for our binary response is no.

```{r}
#| label: Find out the baseline category.
levels(email$yesno)
```

This means that estimates from the logistic regression model are for a change on the log-odds scale for yes in comparison to the response baseline no.

We can extract the estimated coefficients to report the fitted model as follows. The fitted linear model 2 is \begin{align}
\ln\left(\frac{p}{1-p}\right) &= \alpha + \beta_{crl.tot} \cdot \textrm{crl.tot} +\beta_{dollar} \cdot \textrm{dollar} +\beta_{bang} \cdot \textrm{bang} +\nonumber\\&\hspace{0.5cm}\beta_{money} \cdot \textrm{money} +\beta_{n000} \cdot \textrm{n000} \nonumber\\&= `r mod2coefs[1]` + `r mod2coefs[2]` \cdot \textrm{crl.tot} +`r mod2coefs[3]` \cdot \textrm{dollar} +`r mod2coefs[4]` \cdot \textrm{bang} +\nonumber\\&`r mod2coefs[5]` \cdot \textrm{money} +`r mod2coefs[6]`\cdot \textrm{n000} \nonumber
\end{align} where $p=\text{Prob}(yes)$ and $1-p=\text{Prob}(no)$. Hence, the log-odds of the instructor being spam increase by 0.443 for every one unit increase in scaled total length of uninterrupted sequences of capitals, when keeping other variables constant. Similarly, for a percentage of total number of characters for dollar sign, exclamation mark, money, and 000 for every one unit increase separately, the log-odds of the instructor being spam increase by 7.759, 2.732, 6.039, and 2.926.

This provides us with a point estimate of how the log-odds changes with explanatory variables, however, we are also interested in producing a 95% confidence interval for these log-odds.

```{r}
#| label: tbl-summary_confident_of_model2_coefficient
#| tbl-cap: Summary confident of model2 coefficient.
confint(model2) %>%
  kable()
```

Hence, by @tbl-summary_confident_of_model2_coefficient, the point estimate for crl.tot is 0.44, which has a corresponding 95% confidence interval of (0.23, 0.66). Similarly, the point estimate for dollar is 7.76, with a 95% confidence interval of (4.90, 10.91). For bang, the point estimate is 2.73, with a 95% confidence interval of (2.11, 3.42). For money, the point estimate is 6.04, with a 95% confidence interval of (3.58, 9.22). For n000, the point estimate is 2.93, with a 95% confidence interval of (1.24, 5.14). This can be displayed graphically in the @fig-CI-of-Log-odds.

```{r}
#| label: fig-CI-of-Log-odds
#| fig-cap: 95% CI of confidents in Log-odds.
plot_model(model2, show.values = TRUE, transform = NULL,
           title = "Log-Odds (Email indicator)", show.p = TRUE)
```

## Variable Selection

Variable selection is a crucial step in statistical modeling that aims to identify the most relevant predictors while excluding those that contribute little to the model’s explanatory power. This process helps improve model interpretability, reduce overfitting, and enhance predictive accuracy.

```{r}
#| label: tbl-p_value_Comparation
#| tbl-cap: p value Comparation.
p_values1 <- summary(model1)$coefficients[, "Pr(>|z|)"]
p_values2 <- summary(model2)$coefficients[, "Pr(>|z|)"]

# Create data frames for each model
results1 <- data.frame(Variable = names(p_values1), P_Value_Model1 = p_values1)
results2 <- data.frame(Variable = names(p_values2), P_Value_Model2 = p_values2)

# Merge results by Variable (outer join to keep all variables)
combined_results <- merge(results1, results2, by = "Variable", all = TRUE)

combined_results %>%
  gt() %>%
  fmt_number(columns = c(P_Value_Model1, P_Value_Model2), decimals = 3)
```

In this analysis, variable selection was conducted by examining the p-values of each predictor in two different models. The p-values indicate the statistical significance of each variable, with lower values suggesting stronger evidence against the null hypothesis (i.e., the variable has a significant effect on the response variable). A comparison of p-values from Model 1 and Model 2 is presented in @tbl-p_value_Comparation, where variables with high p-values (e.g., make in Model 1 with p=0.153) were considered for removal, as they do not significantly contribute to the model. After fitting model.

```{r}
#| label: tbl-Devariance_Comparation
#| tbl-cap: Devariance Comparation.
dev_model1 <- deviance(model1)  # Residual deviance of model1
dev_model2 <- deviance(model2)  # Residual deviance of model2
dev<-data.frame(c(dev_model1,dev_model2))
colnames(dev)<-c('Deviance')
dev$Model<-c('Model1','Model2')
gt(dev[2:1])
```

Additionally, the impact of variable selection was assessed by comparing the residual deviance of both models which show in @tbl-Devariance_Comparation. We want to test that $$H_{0}:\beta_{0}=(\beta_{crl.tot},\beta_{dollar},\beta_{bang},\beta_{money},\beta_{n000})^{T}$$ and $$H_{1}:\beta_{1}=(\beta_{crl.tot},\beta_{dollar},\beta_{bang},\beta_{money},\beta_{n000},\beta_{make})^{T}$$ Let $\text{D}_{0}$ the deviance of model 1 and $\text{D}_{1}$ the deviance of model 2. Since $$\text{D}_{0}-\text{D}_{1}=754.0444-752.4441=1.6003<\chi^{2}(0.95,1)=3.84$$we have no significant evidence to reject the null hypothesis. Again we conclude that there is no evidence of lack of fit for the logit model 2. Hence, model 2 is acceptable by compare the deviance.

The final selection of variables balances statistical significance with model complexity, ensuring a robust and interpretable predictive model.

```{r}
#| label: tbl-AIC_BIC
#| tbl-cap: AIC & BIC of two models.
model_comparison <- bind_rows(
  glance(model1) %>% mutate(Model = "model1"),
  glance(model2) %>% mutate(Model = "model2")
) %>% 
  dplyr::select(Model, AIC, BIC)
kable(model_comparison, digits = 2, 
      caption = "Model comparison values for different models")# Print the table with kable
```

In this analysis, AIC and BIC were calculated for two competing models, Model 1 and Model 2. The results, presented in @tbl-AIC_BIC, show that Model 1 has a slightly lower AIC (690.89) compared to Model 2 (691.74). However, BIC for Model 2 (720.11) is slightly lower than for Model 1 (723.99), suggesting that Model 2 may be preferable when considering model complexity.

These results indicate that while Model 1 provides a slightly better overall fit, Model 2 is more parsimonious according to BIC, which includes a stronger penalty for the number of parameters. The final model choice depends on the emphasis placed on model complexity versus fit.

## Odds

We can obtain the odds scale $\frac{p}{1-p}$ by

$$
\frac{p}{1-p}= exp(`r mod2coefs[1]` + `r mod2coefs[2]` \cdot \textrm{crl.tot} +`r mod2coefs[3]` \cdot \textrm{dollar} +`r mod2coefs[4]` \cdot \textrm{bang} +`r mod2coefs[5]` \cdot \textrm{money} +`r mod2coefs[6]`\cdot \textrm{n000})
$$

```{r}
#| label: tbl-odds_scale
#| tbl-cap: The odds scale of each variables.
model2 %>%
 coef() %>%
  exp() %>%
  t()%>%
  as.data.frame() %>%
  gt()
  
```

The @tbl-odds_scale shows the odds scale of each variables.

-   On the odds scale, the value of the intercept (0.19) give the odds of a email being spam given all explanatory variables equals 0.

-   For total length of uninterrupted sequences of capitals, we have an odds of 1.56, which indicates that for every 1 unit increase in crl.tot, the probability of the email being spam increase by 56%.

-   For a percentage of total number of occurrences of the dollar sign, we have an odds of 2343.40, which indicates that for every 1 unit increase in dollar, the probability of the email being spam increase by a factor of 234240%.

-   For a percentage of total number of occurrences of the exclamation mark, we have an odds of 15.36, which indicates that for every 1 unit increase in bang, the probability of the email being spam increase by a factor of 1436%.

-   For a percentage of total number of occurrences of money, we have an odds of 419.31, which indicates that for every 1 unit increase in money, the probability of the email being spam increase by a factor of 41831%.

-   For a percentage of total number of occurrences of '000', we have an odds of 18.65, which indicates that for every 1 unit increase in n000, the odds of the email being spam increase by a factor of 1765%.

The corresponding 95% confidence interval of odds can be displayed graphically in @fig-CI-of-odds-scale.

```{r}
#| label: fig-CI-of-odds-scale
#| fig-cap: 95% CI of confidents in odds.
plot_model(model2, show.values = TRUE,
           title = "Odds (Email indicator)", show.p = FALSE)
```

## Probabilities

We can obtain the probability $p=\text{Prob}(\text{spam})$ by $$
p=\frac{exp(`r mod2coefs[1]` + `r mod2coefs[2]` \cdot \textrm{crl.tot} +`r mod2coefs[3]` \cdot \textrm{dollar} +`r mod2coefs[4]` \cdot \textrm{bang} +`r mod2coefs[5]` \cdot \textrm{money} +`r mod2coefs[6]`\cdot \textrm{n000})}{1+exp(`r mod2coefs[1]` + `r mod2coefs[2]` \cdot \textrm{crl.tot} +`r mod2coefs[3]` \cdot \textrm{dollar} +`r mod2coefs[4]` \cdot \textrm{bang} +`r mod2coefs[5]` \cdot \textrm{money} +`r mod2coefs[6]`\cdot \textrm{n000})}$$

# Conclusion

In short, this analysis are trying to find out the relationship between the certain words and characters in the email and spam, and fit a model for it.

First, correlation for each characters and words are shown in @fig-pairs . Also, the range of each variable before and after scaling are shown in @tbl-range and @tbl-scaled_range . Boxplots for each variable are explored.

Then, a generalized linear model is fitted for classifying non-spam and spam. After model selection, model 2 with total length of uninterrupted sequences of capitals, word 'dollar', word 'money', character '!', and character '000' are selected. Model 2 effectively identifies the key characteristics of spam emails, providing insights that can improve the company's spam filter. The result of odds ratio are presented in @tbl-odds_scale . To assess the model, hypothesis test by deviance are used and compared in @tbl-Devariance_Comparation . AIC and BIC are also concerned in @tbl-AIC_BIC . By refining the model and removing non-essential predictors, we ensure a more accurate and efficient spam detection system, reducing false positives while effectively blocking unwanted emails.
